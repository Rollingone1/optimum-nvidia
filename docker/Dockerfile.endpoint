FROM nvcr.io/nvidia/tritonserver:24.02-trtllm-python-py3

LABEL maintainer="Morgan Funtowicz <morgan@hf.co>"

ARG VCS_REF
ARG BUILD_DATE
ARG BUILD_VERSION

# Triton Inference Server specifics args
ARG TOKENIZER_FOLDER=/opt/endpoints/tokenizer

LABEL org.label-schema.schema-version="1.0"
LABEL org.label-schema.name="huggingface/inference-endpoints-trtllm"
LABEL org.label-schema.build-date=$BUILD_DATE
LABEL org.label-schema.version=$BUILD_VERSION
LABEL org.label-schema.vcs-ref=$VCS_REF
LABEL org.label-schema.vendor="Hugging Face Inc."
LABEL org.label-schema.version="1.0.0"
LABEL org.label-schema.url="https://hf.co/hardware"
LABEL org.label-schema.vcs-url="https://github.com/huggingface/optimum-nvidia"
LABEL org.label-schema.decription="Hugging Face Inference Server docker image for TensorRT-LLM Inference"


# Expose (in-order) HTTP, GRPC, Metrics endpoints
EXPOSE 8000/tcp
EXPOSE 8001/tcp
EXPOSE 8002/tcp

# Create the Triton Inference Server layout
COPY templates/triton /opt/endpoints

# Processing layer
RUN ln -s /repository/tokenizer.json /opt/endpoints/tokenizer/tokenizer.json && \
    ln -s /repository/special_tokens_map.json /opt/endpoints/tokenizer/special_tokens_map.json && \
    ln -s /repository/tokenizer_config.json /opt/endpoints/tokenizer/tokenizer_config.json

# Modeling layer
RUN ln -s /repository/engines/config.json /opt/endpoints/llm/1/config.json && \
    ln -s /repository/engines/*.engine /opt/endpoints/llm/1/

# Inference-endpoint mount the content of the repository in /repository, so let's place ourselves here
WORKDIR /repository

CMD ["mpirun", "--allow-run-as-root", "-n", "1", "/opt/tritonserver/bin/tritonserver", "--exit-on-error=false", "--model-repo=/opt/endpoint"]