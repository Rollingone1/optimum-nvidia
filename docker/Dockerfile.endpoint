FROM nvcr.io/nvidia/tritonserver:24.02-trtllm-python-py3

LABEL maintainer="Morgan Funtowicz <morgan@hf.co>"

ARG VCS_REF
ARG BUILD_DATE
ARG BUILD_VERSION

# Runtime parameters
ARG ALLOW_STREAMING="true"
ARG BATCH_SCHEDULER_POLICY="max_utilization"
ARG KV_CACHE_RESERVED_MEMORY_FRACTION="0.9"

ENV ARG_ALLOW_STREAMING=$ALLOW_STREAMING
ENV ARG_BATCH_SCHEDULER_POLICY=$BATCH_SCHEDULER_POLICY
ENV ARG_KV_CACHE_RESERVED_MEMORY_FRACTION=$KV_CACHE_RESERVED_MEMORY_FRACTION

LABEL org.label-schema.schema-version="1.0"
LABEL org.label-schema.name="huggingface/inference-endpoints-trtllm"
LABEL org.label-schema.build-date=$BUILD_DATE
LABEL org.label-schema.version=$BUILD_VERSION
LABEL org.label-schema.vcs-ref=$VCS_REF
LABEL org.label-schema.vendor="Hugging Face Inc."
LABEL org.label-schema.version="1.0.0"
LABEL org.label-schema.url="https://hf.co/hardware"
LABEL org.label-schema.vcs-url="https://github.com/huggingface/optimum-nvidia"
LABEL org.label-schema.decription="Hugging Face Inference Server docker image for TensorRT-LLM Inference"


RUN apt update && \
    apt upgrade -y && \
    apt install -y jq && \
    rm -rf /var/lib/apt/lists/*


# Expose (in-order) HTTP, GRPC, Metrics endpoints
EXPOSE 8000/tcp
EXPOSE 8001/tcp
EXPOSE 8002/tcp

# Inference-endpoint mount the content of the repository in /repository, so let's place ourselves here
WORKDIR /opt/endpoint

COPY templates/triton/ /opt/endpoint/

# Retrieve the Nvidia Triton launcher script
RUN wget https://raw.githubusercontent.com/triton-inference-server/tensorrtllm_backend/main/scripts/launch_triton_server.py

# Copy our Inference Endpoint specific start script
COPY docker/scripts/hf_endpoint_start.sh /opt/endpoint/start.sh
RUN chmod +x /opt/endpoint/start.sh

# Run
CMD /opt/endpoint/start.sh