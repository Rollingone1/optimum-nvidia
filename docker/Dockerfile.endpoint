FROM nvcr.io/nvidia/tritonserver:24.02-trtllm-python-py3

LABEL maintainer="Morgan Funtowicz <morgan@hf.co>"

ARG VCS_REF
ARG BUILD_DATE
ARG BUILD_VERSION

# Triton Inference Server specifics args
ARG TOKENIZER_FOLDER=/opt/endpoints/tokenizer

LABEL org.label-schema.schema-version="1.0"
LABEL org.label-schema.name="huggingface/inference-endpoints-trtllm"
LABEL org.label-schema.build-date=$BUILD_DATE
LABEL org.label-schema.version=$BUILD_VERSION
LABEL org.label-schema.vcs-ref=$VCS_REF
LABEL org.label-schema.vendor="Hugging Face Inc."
LABEL org.label-schema.version="1.0.0"
LABEL org.label-schema.url="https://hf.co/hardware"
LABEL org.label-schema.vcs-url="https://github.com/huggingface/optimum-nvidia"
LABEL org.label-schema.decription="Hugging Face Inference Server docker image for TensorRT-LLM Inference"


# Expose (in-order) HTTP, GRPC, Metrics endpoints
EXPOSE 8000/tcp
EXPOSE 8001/tcp
EXPOSE 8002/tcp

# Inference-endpoint mount the content of the repository in /repository, so let's place ourselves here
WORKDIR /opt/endpoint

COPY templates/triton/postprocessing/1/model.py /opt/endpoint/postprocessing/1/model.py
COPY templates/triton/preprocessing/1/model.py /opt/endpoint/preprocessing/1/model.py

# Retrieve the Nvidia Triton launcher script
RUN wget https://raw.githubusercontent.com/triton-inference-server/tensorrtllm_backend/main/scripts/launch_triton_server.py

# Copy our Inference Endpoint specific start script
COPY docker/scripts/hf_endpoint_start.sh /opt/endpoint/start.sh
RUN chmod +x /opt/endpoint/start.sh

# Run
CMD /opt/endpoint/start.sh